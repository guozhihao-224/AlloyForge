use af_models::qwen2::Qwen2Model;
use af_ops::sampling;
use af_tokenizer::Tokenizer;
use anyhow::Result;
use candle_core::{DType, Device, Tensor};

#[test]
#[ignore] // éœ€è¦çœŸå®æ¨¡å‹ï¼Œä½¿ç”¨ `-- --ignored` è¿è¡Œ
fn test_qwen2_generate() -> Result<()> {
    let model_path = std::env::var("QWEN2_MODEL_PATH")
        .unwrap_or_else(|_| "./../../Qwen2-0.5B".to_string());

    println!("ğŸ”§ Loading model from: {}", model_path);

    // 1. åŠ è½½æ¨¡å‹å’Œ tokenizer
    let device = Device::Cpu;
    let mut model = Qwen2Model::from_pretrained(&model_path, DType::F32, &device)?;
    println!("âœ… Model loaded");

    let tokenizer_path = format!("{}/tokenizer.json", model_path);
    let tokenizer = Tokenizer::from_file(&tokenizer_path)?;
    println!("âœ… Tokenizer loaded");
    println!("   Vocab size: {}", tokenizer.vocab_size());
    println!("   EOS token ID: {:?}", tokenizer.eos_token_id());

    // 2. Tokenize prompt
    let prompt = "ä½ å¥½";
    println!("\nğŸ¯ Prompt: \"{}\"", prompt);
    
    let input_ids = tokenizer.encode(prompt, false)?;
    println!("ğŸ“ Encoded tokens: {:?}", input_ids);
    
    let input_tensor = Tensor::new(input_ids.as_slice(), &device)?.unsqueeze(0)?;
    println!("   Input shape: {:?}", input_tensor.shape());

    // 3. Prefill é˜¶æ®µ
    println!("\nğŸ”„ Prefill stage...");
    let mut logits = model.forward(&input_tensor, 0)?;
    println!("âœ… Prefill done, logits shape: {:?}", logits.shape());

    // 4. Decode å¾ªç¯
    println!("\nğŸ”„ Decode stage...");
    let mut generated_ids = input_ids.clone();
    let max_new_tokens = 1000;

    for i in 0..max_new_tokens {
        // è·å–æœ€åä¸€ä¸ª token çš„ logits
        let seq_len = logits.dim(1)?;
        let last_logits = logits
            .narrow(1, seq_len - 1, 1)?  // [batch, 1, vocab]
            .squeeze(0)?                  // [1, vocab]
            .squeeze(0)?;                 // [vocab]

        // é‡‡æ ·ä¸‹ä¸€ä¸ª token
        let logits_vec = last_logits.to_vec1::<f32>()?;
        let next_token = sampling::greedy(&logits_vec)
            .ok_or_else(|| anyhow::anyhow!("Failed to sample token"))?;

        // æ£€æŸ¥ EOS
        if Some(next_token) == tokenizer.eos_token_id() {
            println!("ğŸ›‘ EOS token detected at step {}", i);
            break;
        }

        generated_ids.push(next_token);

        // éƒ¨åˆ†è§£ç æ˜¾ç¤ºè¿›åº¦
        if i % 5 == 0 || i < 5 {
            let partial = tokenizer.decode(&generated_ids, false)?;
            println!("   Step {}: {}", i, partial);
        }

        // Forward ä¸‹ä¸€ä¸ª token
        let next_tensor = Tensor::new(&[next_token], &device)?.unsqueeze(0)?;
        let position = input_ids.len() + i;
        logits = model.forward(&next_tensor, position)?;
    }

    // 5. æœ€ç»ˆè§£ç 
    println!("\nğŸ“¤ Final output:");
    let output = tokenizer.decode(&generated_ids, true)?;
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("{}", output);
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("\nğŸ“Š Stats:");
    println!("   Input tokens: {}", input_ids.len());
    println!("   Generated tokens: {}", generated_ids.len() - input_ids.len());
    println!("   Total tokens: {}", generated_ids.len());

    // éªŒè¯ç”Ÿæˆäº†æ–°å†…å®¹
    assert!(
        generated_ids.len() > input_ids.len(),
        "Model should generate new tokens"
    );
    assert!(output.len() >= prompt.len(), "Output should not be shorter than input");

    println!("\nâœ… Test passed!");
    Ok(())
}

#[test]
#[ignore]
fn test_qwen2_generate_english() -> Result<()> {
    let model_path = std::env::var("QWEN2_MODEL_PATH")
        .unwrap_or_else(|_| "./../../Qwen2-0.5B".to_string());

    println!("ğŸ”§ Loading model from: {}", model_path);

    let device = Device::Cpu;
    let mut model = Qwen2Model::from_pretrained(&model_path, DType::F32, &device)?;
    let tokenizer = Tokenizer::from_file(format!("{}/tokenizer.json", model_path))?;

    println!("âœ… Model and tokenizer loaded");

    // è‹±æ–‡ prompt
    let prompt = "Once upon a time";
    println!("\nğŸ¯ Prompt: \"{}\"", prompt);

    let input_ids = tokenizer.encode(prompt, false)?;
    let input_tensor = Tensor::new(input_ids.as_slice(), &device)?.unsqueeze(0)?;

    // Prefill
    let mut logits = model.forward(&input_tensor, 0)?;

    // Decode
    let mut generated_ids = input_ids.clone();
    for i in 0..50 {
        let seq_len = logits.dim(1)?;
        let last_logits = logits
            .narrow(1, seq_len - 1, 1)?
            .squeeze(0)?
            .squeeze(0)?;

        let logits_vec = last_logits.to_vec1::<f32>()?;
        let next_token = sampling::greedy(&logits_vec)
            .ok_or_else(|| anyhow::anyhow!("Failed to sample"))?;

        if Some(next_token) == tokenizer.eos_token_id() {
            break;
        }

        generated_ids.push(next_token);

        let next_tensor = Tensor::new(&[next_token], &device)?.unsqueeze(0)?;
        logits = model.forward(&next_tensor, input_ids.len() + i)?;
    }

    let output = tokenizer.decode(&generated_ids, true)?;
    println!("\nğŸ“¤ Generated:\n{}", output);

    assert!(generated_ids.len() > input_ids.len());
    println!("\nâœ… English generation test passed!");
    Ok(())
}

