use anyhow::Result;
use candle_core::{DType, Device};

#[test]
#[ignore] // éœ€è¦å®é™…çš„æ¨¡å‹æ–‡ä»¶
fn test_qwen2_load_model() -> Result<()> {
    // è¿™ä¸ªæµ‹è¯•éœ€è¦çœŸå®çš„ Qwen2-0.5B æ¨¡å‹æ–‡ä»¶
    // ä¸‹è½½æ¨¡å‹ï¼šhuggingface-cli download Qwen/Qwen2-0.5B --local-dir /path/to/Qwen2-0.5B

    let model_path =
        std::env::var("QWEN2_MODEL_PATH").unwrap_or_else(|_| "/path/to/Qwen2-0.5B".to_string());

    if !std::path::Path::new(&model_path).exists() {
        eprintln!("âš ï¸  Model not found at {}", model_path);
        eprintln!("   Set QWEN2_MODEL_PATH environment variable to test with real model");
        return Ok(());
    }

    let device = Device::Cpu;
    let dtype = DType::BF16;

    println!("ğŸ“¦ Loading Qwen2 model from {}...", model_path);
    let mut model = af_models::qwen2::Qwen2Model::from_pretrained(&model_path, dtype, &device)?;

    println!("âœ… Model loaded successfully!");
    println!("   Config: {:?}", model.config());

    // æµ‹è¯• forward
    let input_ids = candle_core::Tensor::new(&[151643u32, 108386u32, 151645u32], &device)?;
    let input_ids = input_ids.unsqueeze(0)?; // [1, seq_len]

    println!("ğŸ”„ Running forward pass...");
    let logits = model.forward(&input_ids, 0)?;

    let (_batch, seq_len, vocab_size) = logits.dims3()?;
    println!("âœ… Forward pass successful!");
    println!("   Output shape: [1, {}, {}]", seq_len, vocab_size);

    assert_eq!(vocab_size, model.config().vocab_size);

    Ok(())
}

#[test]
fn test_qwen2_config_loading() -> Result<()> {
    // æµ‹è¯•é…ç½®åŠ è½½ï¼ˆä¸éœ€è¦æ¨¡å‹æ–‡ä»¶ï¼‰
    use af_models::qwen2::Qwen2Config;

    // åˆ›å»ºä¸€ä¸ªä¸´æ—¶é…ç½®æ–‡ä»¶
    let temp_dir = std::env::temp_dir();
    let config_path = temp_dir.join("test_qwen2_config.json");

    let config_json = r#"{
        "hidden_size": 896,
        "intermediate_size": 4864,
        "num_attention_heads": 14,
        "num_hidden_layers": 24,
        "num_key_value_heads": 2,
        "rms_norm_eps": 1e-06,
        "rope_theta": 1000000.0,
        "vocab_size": 151936,
        "max_position_embeddings": 32768,
        "tie_word_embeddings": true,
        "bos_token_id": 151643,
        "eos_token_id": 151645,
        "torch_dtype": "bfloat16"
    }"#;

    std::fs::write(&config_path, config_json)?;

    let config = Qwen2Config::from_file(&config_path)?;

    assert_eq!(config.hidden_size, 896);
    assert_eq!(config.num_attention_heads, 14);
    assert_eq!(config.head_dim(), 64);
    assert_eq!(config.num_kv_groups(), 7);

    // æ¸…ç†
    std::fs::remove_file(&config_path)?;

    println!("âœ… Qwen2 config loading test passed");
    Ok(())
}
